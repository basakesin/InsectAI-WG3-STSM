{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basakesin/InsectAI-WG3-STSM/blob/main/test_model_with_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Your Model (Keras & PyTorch)\n",
        "\n",
        "This notebook lets you **upload a trained image classification model** and **interactively test predictions** via a simple **Gradio web interface** ‚Äî no extra coding required.  \n",
        "It supports both **TensorFlow/Keras** and **PyTorch** models.\n",
        "\n",
        "\n",
        "## What You Can Load\n",
        "- **Keras / TensorFlow**\n",
        "  - `.keras`, `.h5`, or a `.zip` that contains a **SavedModel**  \n",
        "  - Optional: `class_names.txt` (one label per line) to map outputs to readable names  \n",
        "  - Built-in support for common backbones: **EfficientNetB0**, **MobileNetV2**, **ResNet50**, **InceptionV3**\n",
        "\n",
        "- **PyTorch**\n",
        "  - `.pt` / `.pth` **full model** (`torch.save(model, ...)`)  \n",
        "  - `.pt` / `.pth` **state_dict** (`torch.save(model.state_dict(), ...)`)  \n",
        "  - Optional: `class_names.txt` for readable labels  \n",
        "  - If you upload a **state_dict**, you must select the correct **Backbone** in the UI (**ResNet50**, **MobileNetV2**, **EfficientNetB0**, **InceptionV3**)\n",
        "\n",
        "\n",
        "\n",
        "## Key Features\n",
        "- **Upload & run** your trained model (Keras or PyTorch) in one place  \n",
        "- **Optional class names** for human-readable outputs  \n",
        "- **Automatic preprocessing**  \n",
        "  - Keras ‚Üí detects internal `preprocess_input` if present; otherwise applies the correct external preprocess (or `/255`)  \n",
        "  - PyTorch ‚Üí standard **ImageNet normalization (mean/std)**  \n",
        "- **Drag-and-drop image testing**  \n",
        "- **Interactive results**: top-5 predictions with confidence scores  \n",
        "- **Binary models**: outputs shown as **positive/negative probabilities**\n",
        "\n",
        "\n",
        "\n",
        "## Quick Start\n",
        "1. In the Colab menu, go to **Runtime ‚Üí Run all**  \n",
        "   - This installs dependencies, prepares the notebook, and launches the Gradio app automatically\n",
        "2. In the UI:\n",
        "   - Select **Framework** ‚Üí Keras or PyTorch  \n",
        "   - **Upload your model file**  \n",
        "     - Keras: `.keras`, `.h5`, or `.zip`  \n",
        "     - PyTorch: `.pt` / `.pth`  \n",
        "   - (Optional) Upload `class_names.txt`  \n",
        "   - If using a PyTorch **state_dict**, choose the right **Backbone**\n",
        "3. After setup, a **Gradio link** will appear  \n",
        "   - Click it to open the interactive web interface  \n",
        "   - Upload a test image ‚Üí see predictions instantly\n",
        "\n",
        "\n",
        "\n",
        "## Notes & Tips\n",
        "- **Input size**: InceptionV3 ‚Üí `299√ó299`; others ‚Üí `224√ó224` (handled automatically)  \n",
        "- **Binary outputs**: If the model returns a single logit, the app applies **sigmoid** and shows `positive` / `negative` probabilities  \n",
        "- **SavedModel ZIPs**: The archive must contain `saved_model.pb` at the top level  \n",
        "- **Common issues**:  \n",
        "  - Wrong backbone for PyTorch `state_dict` ‚Üí select the correct one  \n",
        "  - `class_names.txt` length doesn‚Äôt match model outputs ‚Üí fallback to `class_0, class_1, ...`  \n",
        "  - Environment glitches ‚Üí use **Runtime ‚Üí Restart and run all** to reset\n",
        "\n",
        "\n",
        "\n",
        "üí° **That‚Äôs it!** Upload your model, select the right options, and start testing predictions interactively.\n"
      ],
      "metadata": {
        "id": "kPnPVduSQtHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSD-ddOHOtrz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Load Required Packages\n",
        "%pip -q install \"tensorflow==2.19.0\" \"tf-keras==2.19.0\" \"keras==3.5.0\" \"numpy==2.0.2\" \"Pillow>=10.3.0\"\n",
        "import tensorflow as tf, keras, numpy as np\n",
        "import os, io, zipfile, tempfile, json\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "print(\"TF\", tf.__version__, \"| Keras\", keras.__version__, \"| NumPy\", np.__version__)\n",
        "# Beklenen: TF 2.19.0 | Keras 3.5.x | NumPy 2.0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDrN3jEUQJvk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run Your Interface (Keras + PyTorch)\n",
        "import os, io, zipfile, tempfile, json\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# ====== KERAS / TF ======\n",
        "import tensorflow as tf, keras\n",
        "# Allow unsafe deserialization (for trusted models with Lambda/preprocess layers)\n",
        "try:\n",
        "    keras.config.enable_unsafe_deserialization()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# DepthwiseConv2D \"groups\" compatibility patch\n",
        "from tensorflow.keras.layers import DepthwiseConv2D as _TFDepthwiseConv2D\n",
        "class DepthwiseConv2DCompat(_TFDepthwiseConv2D):\n",
        "    def __init__(self, *args, groups=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "CUSTOM_OBJECTS_BASE = {\"DepthwiseConv2D\": DepthwiseConv2DCompat}\n",
        "\n",
        "# Candidate Keras preprocess functions\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as eff_pre\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mob_pre\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as res_pre\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input as inc_pre\n",
        "\n",
        "KERAS_PREPROC = {\n",
        "    \"EfficientNetB0\": eff_pre,\n",
        "    \"MobileNetV2\":   mob_pre,\n",
        "    \"ResNet50\":      res_pre,\n",
        "    \"InceptionV3\":   inc_pre,\n",
        "}\n",
        "\n",
        "def _infer_input_size(model):\n",
        "    ishape = model.input_shape\n",
        "    if isinstance(ishape, list): ishape = ishape[0]\n",
        "    _, h, w, c = ishape\n",
        "    if h is None or w is None:\n",
        "        h, w = 224, 224\n",
        "    return (w, h), c\n",
        "\n",
        "def _prep_image_keras(pil_img, size, channels, preprocess=None, scale_255=True):\n",
        "    img = pil_img.convert(\"RGB\").resize(size)\n",
        "    x = np.array(img, dtype=np.float32)\n",
        "    if channels == 1:\n",
        "        x = np.mean(x, axis=-1, keepdims=True)\n",
        "    if preprocess is not None:\n",
        "        x = preprocess(x)  # do not divide by 255 here\n",
        "    else:\n",
        "        if scale_255:\n",
        "            x = x / 255.0\n",
        "    return np.expand_dims(x, 0)\n",
        "\n",
        "def _try_load_no_custom(path_or_dir):\n",
        "    co = dict(CUSTOM_OBJECTS_BASE)\n",
        "    return tf.keras.models.load_model(\n",
        "        path_or_dir, compile=False, safe_mode=False, custom_objects=co\n",
        "    )\n",
        "\n",
        "def _read_class_names(txt_path: str):\n",
        "    if not txt_path:\n",
        "        return None\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        names = [ln.strip() for ln in f if ln.strip()]\n",
        "    return names or None\n",
        "\n",
        "def _resolve_zip_savedmodel(path):\n",
        "    workdir = tempfile.mkdtemp()\n",
        "    with zipfile.ZipFile(path, \"r\") as zf:\n",
        "        zf.extractall(workdir)\n",
        "    cand = None\n",
        "    for root, _, files in os.walk(workdir):\n",
        "        if \"saved_model.pb\" in files:\n",
        "            cand = root; break\n",
        "    if not cand:\n",
        "        raise RuntimeError(\"No 'saved_model.pb' found inside ZIP.\")\n",
        "    return cand\n",
        "\n",
        "def keras_load_model_any(model_path: str, backbone_hint: Optional[str]):\n",
        "    low = model_path.lower()\n",
        "    target = _resolve_zip_savedmodel(model_path) if low.endswith(\".zip\") else model_path\n",
        "\n",
        "    # 1) Try without preprocess (means no Lambda inside the model)\n",
        "    try:\n",
        "        m = _try_load_no_custom(target)\n",
        "        return m, None, None, False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Try with candidate preprocess functions (prioritize selected backbone)\n",
        "    order = []\n",
        "    if backbone_hint in KERAS_PREPROC:\n",
        "        order.append((backbone_hint, KERAS_PREPROC[backbone_hint]))\n",
        "    for name, fn in KERAS_PREPROC.items():\n",
        "        if name != backbone_hint:\n",
        "            order.append((name, fn))\n",
        "\n",
        "    last_err = None\n",
        "    for name, fn in order:\n",
        "        try:\n",
        "            co = dict(CUSTOM_OBJECTS_BASE); co[\"preprocess_input\"] = fn\n",
        "            m = tf.keras.models.load_model(target, compile=False, safe_mode=False, custom_objects=co)\n",
        "            return m, name, fn, True\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise RuntimeError(f\"Keras model could not be deserialized. Last error: {last_err}\")\n",
        "\n",
        "# ====== PYTORCH ======\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models as tvm\n",
        "from torchvision import transforms as T\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "def _canon(name: str) -> str:\n",
        "    \"\"\"Normalize names: 'MobileNetV2' -> 'mobilenetv2', 'EfficientNet_B0' -> 'efficientnetb0'\"\"\"\n",
        "    return \"\".join(ch for ch in (name or \"\").lower() if ch.isalnum())\n",
        "\n",
        "def _torch_default_input_size(backbone: str) -> Tuple[int,int]:\n",
        "    key = _canon(backbone)\n",
        "    return (299, 299) if key == \"inceptionv3\" else (224, 224)\n",
        "\n",
        "def _torch_transform(size: Tuple[int,int]):\n",
        "    w, h = size\n",
        "    return T.Compose([\n",
        "        T.Resize((h, w)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "def _torch_build_backbone(name: str, num_classes: Optional[int] = None):\n",
        "    key = _canon(name)\n",
        "    if key == \"resnet50\":\n",
        "        m = tvm.resnet50(weights=None)\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, num_classes or m.fc.out_features)\n",
        "    elif key == \"mobilenetv2\":\n",
        "        m = tvm.mobilenet_v2(weights=None)\n",
        "        in_feats = m.classifier[-1].in_features\n",
        "        m.classifier[-1] = nn.Linear(in_feats, num_classes or m.classifier[-1].out_features)\n",
        "    elif key == \"efficientnetb0\":\n",
        "        m = tvm.efficientnet_b0(weights=None)\n",
        "        in_feats = m.classifier[-1].in_features\n",
        "        m.classifier[-1] = nn.Linear(in_feats, num_classes or m.classifier[-1].out_features)\n",
        "    elif key == \"inceptionv3\":\n",
        "        m = tvm.inception_v3(weights=None, aux_logits=False)\n",
        "        in_feats = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_feats, num_classes or m.fc.out_features)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported PyTorch backbone: {name}\")\n",
        "    return m\n",
        "\n",
        "def torch_load_model_any(model_path: str, backbone_hint: Optional[str], class_names: Optional[list]):\n",
        "    \"\"\"\n",
        "    Tries in order:\n",
        "      1) torch.jit.load (scripted/trace)\n",
        "      2) torch.load full model object\n",
        "      3) torch.load state_dict -> rebuild backbone (with class count if available)\n",
        "    \"\"\"\n",
        "    # 1) JIT\n",
        "    try:\n",
        "        m = torch.jit.load(model_path, map_location=\"cpu\")\n",
        "        m.eval()\n",
        "        return m, (backbone_hint or \"Unknown\"), _torch_default_input_size(backbone_hint or \"resnet50\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    last_err = None\n",
        "    try:\n",
        "        obj = torch.load(model_path, map_location=\"cpu\")\n",
        "        # Full model?\n",
        "        if hasattr(obj, \"state_dict\") and callable(obj.state_dict):\n",
        "            try: obj.eval()\n",
        "            except Exception: pass\n",
        "            return obj, (backbone_hint or \"Unknown\"), _torch_default_input_size(backbone_hint or \"resnet50\")\n",
        "        # state_dict?\n",
        "        if isinstance(obj, dict):\n",
        "            if backbone_hint is None:\n",
        "                raise RuntimeError(\"State dict detected. Please select a Backbone.\")\n",
        "            num_classes = (len(class_names) if class_names else None)\n",
        "            m = _torch_build_backbone(backbone_hint, num_classes=num_classes)\n",
        "            missing, unexpected = m.load_state_dict(obj, strict=False)\n",
        "            if missing or unexpected:\n",
        "                print(\"State dict load ‚Äî missing:\", missing, \"unexpected:\", unexpected)\n",
        "            m.eval()\n",
        "            return m, backbone_hint, _torch_default_input_size(backbone_hint)\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "\n",
        "    raise RuntimeError(f\"PyTorch model could not be loaded. Last error: {last_err}\")\n",
        "\n",
        "def _torch_predict(model: torch.nn.Module, img: Image.Image, size: Tuple[int,int], class_names: Optional[list]):\n",
        "    tfm = _torch_transform(size)\n",
        "    x = tfm(img.convert(\"RGB\")).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        if isinstance(out, (list, tuple)):\n",
        "            out = out[0]\n",
        "        out = out.squeeze(0)\n",
        "        if out.ndim == 0 or out.numel() == 1:   # binary logit\n",
        "            p = torch.sigmoid(out.flatten()[0]).item()\n",
        "            return {\"positive\": float(p), \"negative\": float(1.0 - p)}\n",
        "        # multiclass\n",
        "        probs = torch.softmax(out, dim=-1).cpu().numpy()\n",
        "        if class_names and len(class_names) == probs.shape[-1]:\n",
        "            return {cls: float(p) for cls, p in zip(class_names, probs)}\n",
        "        return {f\"class_{i}\": float(p) for i, p in enumerate(probs)}\n",
        "\n",
        "# ====== GRADIO UI ======\n",
        "with gr.Blocks(title=\"Image Classification (Keras / PyTorch)\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"## Image Classification (Keras / PyTorch)\\n\"\n",
        "        \"Upload your model and **class_names.txt**. Select the **Framework** and, if necessary, the **Backbone**, then provide an image to get predictions.\\n\\n\"\n",
        "        \"**Supported formats**  \\n\"\n",
        "        \"- **Keras**: `.keras`, `.h5`, or `.zip` containing a SavedModel  \\n\"\n",
        "        \"- **PyTorch**: `.pt` / `.pth` (full model **or** `state_dict`)  \\n\"\n",
        "        \"Note: For `state_dict`, you must select the correct backbone.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        framework_dd = gr.Dropdown(\n",
        "            choices=[\"Keras\", \"PyTorch\"],\n",
        "            value=\"Keras\",\n",
        "            label=\"Framework\"\n",
        "        )\n",
        "        model_file = gr.File(\n",
        "            label=\"Model File (.keras / .h5 / .zip / .pt / .pth)\",\n",
        "            file_types=[\".keras\", \".h5\", \".zip\", \".pt\", \".pth\"],\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "        class_file = gr.File(\n",
        "            label=\"Class Names (class_names.txt) ‚Äî optional\",\n",
        "            file_types=[\".txt\"],\n",
        "            type=\"filepath\"\n",
        "        )\n",
        "        backbone_dropdown = gr.Dropdown(\n",
        "            choices=[\"EfficientNetB0\", \"MobileNetV2\", \"ResNet50\", \"InceptionV3\"],\n",
        "            label=\"Backbone (for PyTorch state_dict or Keras deserialization)\",\n",
        "            value=\"MobileNetV2\"\n",
        "        )\n",
        "\n",
        "    load_btn = gr.Button(\"Load Model\", variant=\"primary\")\n",
        "    status = gr.Markdown()\n",
        "\n",
        "    # States\n",
        "    framework_state = gr.State()\n",
        "    model_state = gr.State()\n",
        "    input_size_state = gr.State()      # (w,h)\n",
        "    channels_state = gr.State()        # Keras only; None for PyTorch\n",
        "    class_names_state = gr.State()\n",
        "    preprocess_fn_state = gr.State()   # Keras external preprocess fn\n",
        "    backbone_used_state = gr.State()\n",
        "    has_internal_pre_state = gr.State()# Keras only; None for PyTorch\n",
        "\n",
        "    with gr.Row():\n",
        "        image_in = gr.Image(type=\"pil\", label=\"Input Image\")\n",
        "        predict_btn = gr.Button(\"Predict\", variant=\"primary\")\n",
        "\n",
        "    label_out = gr.Label(num_top_classes=5, label=\"Top-5 Predictions\")\n",
        "\n",
        "    # ------- LOAD -------\n",
        "    def on_load(framework, model_path, class_path, backbone_hint):\n",
        "        if not model_path:\n",
        "            return (\"Please choose a model file.\",\n",
        "                    framework, None, None, None, None, None, None, None)\n",
        "\n",
        "        classes = _read_class_names(class_path)\n",
        "\n",
        "        if framework == \"Keras\":\n",
        "            try:\n",
        "                m, used_backbone, used_pre, has_internal = keras_load_model_any(model_path, backbone_hint)\n",
        "                (w, h), c = _infer_input_size(m)\n",
        "                msg = f\"‚úÖ [Keras] Model loaded. Expected input: {(h,w,c)}\"\n",
        "                if has_internal:\n",
        "                    msg += \" | Preprocessing: inside model (no external /255).\"\n",
        "                else:\n",
        "                    msg += f\" | External preprocessing: {used_backbone or '/255'}\"\n",
        "                if classes:\n",
        "                    msg += f\" | {len(classes)} classes loaded.\"\n",
        "                else:\n",
        "                    msg += \" | No class_names.txt found; class_0, class_1, ... will be used.\"\n",
        "                preprocess_for_runtime = None if has_internal else used_pre\n",
        "                return (msg, framework, m, (w, h), c, classes, preprocess_for_runtime, (used_backbone or \"/255\"), has_internal)\n",
        "            except Exception as e:\n",
        "                return (f\"‚ùå Keras model failed to load: {e}\", framework, None, None, None, None, None, None, None)\n",
        "\n",
        "        # PyTorch\n",
        "        try:\n",
        "            m, used_bb, size = torch_load_model_any(model_path, backbone_hint, classes)\n",
        "            w, h = size\n",
        "            msg = f\"‚úÖ [PyTorch] Model loaded. Expected input: {(h,w,3)} | Backbone: {used_bb}\"\n",
        "            if classes:\n",
        "                msg += f\" | {len(classes)} classes loaded.\"\n",
        "            else:\n",
        "                msg += \" | No class_names.txt found; class_0, class_1, ... will be used.\"\n",
        "            return (msg, framework, m, (w, h), None, classes, None, used_bb, None)\n",
        "        except Exception as e:\n",
        "            return (f\"‚ùå PyTorch model failed to load: {e}\", framework, None, None, None, None, None, None, None)\n",
        "\n",
        "    load_btn.click(\n",
        "        on_load,\n",
        "        inputs=[framework_dd, model_file, class_file, backbone_dropdown],\n",
        "        outputs=[status, framework_state, model_state, input_size_state, channels_state,\n",
        "                 class_names_state, preprocess_fn_state, backbone_used_state, has_internal_pre_state]\n",
        "    )\n",
        "\n",
        "    # ------- PREDICT -------\n",
        "    def on_predict(img, framework, model, input_size, channels, class_names, preprocess_fn, backbone_used, has_internal_pre):\n",
        "        if model is None:\n",
        "            return {\"error\": 1.0}\n",
        "        if img is None:\n",
        "            return {\"no_image\": 1.0}\n",
        "\n",
        "        if framework == \"Keras\":\n",
        "            try:\n",
        "                x = _prep_image_keras(\n",
        "                    img, input_size, channels,\n",
        "                    preprocess=(None if has_internal_pre else preprocess_fn),\n",
        "                    scale_255=(False if has_internal_pre else (preprocess_fn is None))\n",
        "                )\n",
        "                y = model.predict(x, verbose=0)\n",
        "                y = y[0] if isinstance(y, (list, tuple)) else y\n",
        "                y = np.array(y).reshape(-1)\n",
        "\n",
        "                # Binary case\n",
        "                if y.shape == () or y.shape == (1,):\n",
        "                    p = float(y if y.shape == () else y[0])\n",
        "                    if p < 0 or p > 1:  # looks like a logit, apply sigmoid\n",
        "                        p = 1 / (1 + np.exp(-p))\n",
        "                    return {\"positive\": p, \"negative\": 1.0 - p}\n",
        "\n",
        "                # Multiclass case\n",
        "                if class_names and len(class_names) == y.shape[-1]:\n",
        "                    return {cls: float(p) for cls, p in zip(class_names, y)}\n",
        "                return {f\"class_{i}\": float(p) for i, p in enumerate(y)}\n",
        "            except Exception as e:\n",
        "                return {f\"error: {e}\": 1.0}\n",
        "\n",
        "        # PyTorch\n",
        "        try:\n",
        "            return _torch_predict(model, img, input_size, class_names)\n",
        "        except Exception as e:\n",
        "            return {f\"error: {e}\": 1.0}\n",
        "\n",
        "    predict_btn.click(\n",
        "        on_predict,\n",
        "        inputs=[image_in, framework_state, model_state, input_size_state, channels_state,\n",
        "                class_names_state, preprocess_fn_state, backbone_used_state, has_internal_pre_state],\n",
        "        outputs=[label_out]\n",
        "    )\n",
        "\n",
        "demo.queue().launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtROgVuQA9kE1IH3ew7WOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}